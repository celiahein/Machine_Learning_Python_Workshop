{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea25cdf7-bdbc-3cf1-0737-bc51675e3374",
    "_uuid": "fed5696c67bf55a553d6d04313a77e8c617cad99"
   },
   "source": [
    "# Titanic Competition (https://www.kaggle.com/competitions/titanic)\n",
    "\n",
    "\n",
    "Titanic sank on its maiden voyage from England to New York in 1912. Many people died in the tragety. This competition is aimed to predict the survivals of people as accurate as possible based on their information (age, gender, fare, cabin, etc). Please visit https://www.kaggle.com/competitions/titanic fore more information.\n",
    "\n",
    "## General workflow of developing machine learning solutions\n",
    "\n",
    "1. Question or problem definition.\n",
    "2. Acquire training and testing data.\n",
    "3. Data exploration\n",
    "4. Data preprocessing (handling missing data and outliers, encoding categorical features, feature selection and engineering)\n",
    "5. Train and evaluate various models on cleaned data.\n",
    "6. Repeat steps 3-5 to get best results\n",
    "7. Deploy the best trained model (Submit the results obtained by running the model on testing data)\n",
    "\n",
    "## Sklearn classifiers\n",
    "https://scikit-learn.org/stable/supervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5767a33c-8f18-4034-e52d-bf7a8f7d8ab8",
    "_uuid": "847a9b3972a6be2d2f3346ff01fea976d92ecdb6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data analysis and preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Visualization\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning (https://scikit-learn.org/stable/supervised_learning.html)\n",
    "from sklearn.linear_model import LogisticRegression # linear model + sigmoid function\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b5dc743-15b1-aac6-405e-081def6ecca1",
    "_uuid": "2d307b99ee3d19da3c1cddf509ed179c21dec94a"
   },
   "source": [
    "# Load data\n",
    "\n",
    "The data provided from the Kaggle Titanic Competition can be loaded as Pandas' dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7319668-86fe-8adc-438d-0eef3fd0a982",
    "_uuid": "13f38775c12ad6f914254a08f0d1ef948a2bd453"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d6188f3-dc82-8ae6-dabd-83e28fcbf10d",
    "_uuid": "79282222056237a52bbbb1dbd831f057f1c23d69"
   },
   "source": [
    "# Quickly examine the data\n",
    "\n",
    "Please find description of the data at https://www.kaggle.com/c/titanic/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ce473d29-8d19-76b8-24a4-48c217286e42",
    "_uuid": "ef106f38a00e162a80c523778af6dcc778ccc1c2"
   },
   "outputs": [],
   "source": [
    "# To see what features are available in the dataset\n",
    "print(train_df.shape)\n",
    "print(train_df.columns.values)\n",
    "print('-'*80)\n",
    "print(test_df.shape)\n",
    "print(test_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f",
    "_uuid": "e068cd3a0465b65a0930a100cb348b9146d5fd2f"
   },
   "outputs": [],
   "source": [
    "# To display the first n=7 rows of the dataframe\n",
    "train_df.head(5)\n",
    "#print('-'*80)\n",
    "#train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9b805f69-665a-2b2e-f31d-50d87d52865d",
    "_uuid": "817e1cf0ca1cb96c7a28bb81192d92261a8bf427"
   },
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "print('-'*80)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the row with NaN in Fare\n",
    "test_df[test_df.Fare.isnull()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "859102e1-10df-d451-2649-2d4571e5f082",
    "_uuid": "2b7c205bf25979e3242762bfebb0e3eb2fd63010"
   },
   "source": [
    "**What we've learned**\n",
    "- The training dataset has 12 columns while the testing dataset has 11 columns. The testing dataset has one column (survived) missing, which is meant for a trained model to predict.\n",
    "- Some columns are numeric features (int64, float64), others are strings (as numpy objects) which may be interpreted as categorical features later on.\n",
    "- Some columns (*Age, Cabin, Embarked*) have missing values (marked as NaN). There is only one missing value for *Fare* in testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "58e387fe-86e4-e068-8307-70e37fe3f37b",
    "_uuid": "380251a1c1e0b89147d321968dc739b6cc0eecf2"
   },
   "outputs": [],
   "source": [
    "# By default, it only shows numeric columns\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we've learned**\n",
    "\n",
    "- These numeric features are ready to be used in training\n",
    "- PassengerId is not useful in predicting survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show those non-numeric columns\n",
    "train_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.hist(figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5462bc60-258c-76bf-0a73-9adc00a2f493",
    "_uuid": "33bbd1709db622978c0c5879e7c5532d4734ade0"
   },
   "source": [
    "**What we've learned**\n",
    "\n",
    "- Names are unique across the dataset (count=unique=891)\n",
    "- **Sex** has two possible values with 65% male (top=male, freq=577/count=891).\n",
    "- Cabin values have several dupicates across samples. This is probably because multiple passengers shared a cabin.\n",
    "- **Embarked** has three possible values. S port used by most passengers (top=S)\n",
    "- Ticket feature has high ratio (22%) of duplicate values (unique=681)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6db63a30-1d86-266e-2799-dded03c45816",
    "_uuid": "946ee6ca01a3e4eecfa373ca00f88042b683e2ad"
   },
   "source": [
    "# Evaluate feature importance\n",
    "\n",
    "We evaluate feature importance based on its correlation with the target feature (survived)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_df.corr(numeric_only=True)\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above correlation map only shows the correlations between numeric features.\n",
    "Now let's examine the importance of discrete features (int64, object) that have limited number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "68908ba6-bfe9-5b31-cfde-6987fc0fbe9a",
    "_uuid": "00a2f2bca094c5984e6a232c730c8b232e7e20bb"
   },
   "outputs": [],
   "source": [
    "train_df[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0964832a-a4be-2d6f-a89e-63526389cee9",
    "_uuid": "97a845528ce9f76e85055a4bb9e97c27091f6aa1"
   },
   "outputs": [],
   "source": [
    "train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "01c06927-c5a6-342a-5aa8-2e486ec3fd7c",
    "_uuid": "a8f7a16c54417dcd86fc48aeef0c4b240d47d71b"
   },
   "outputs": [],
   "source": [
    "train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e686f98b-a8c9-68f8-36a4-d4598638bbd5",
    "_uuid": "5d953a6779b00b7f3794757dec8744a03162c8fd"
   },
   "outputs": [],
   "source": [
    "train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "87096158-4017-9213-7225-a19aea67a800",
    "_uuid": "892259f68c2ecf64fd258965cff1ecfe77dd73a9"
   },
   "source": [
    "**Features to keep:**\n",
    "- Numeric\n",
    "    - Pclass\n",
    "    - Fare\n",
    "    - Parch\n",
    "    - Age\n",
    "    - SibSp\n",
    "- Categorical\n",
    "    - Sex\n",
    "    - Embarked\n",
    "    \n",
    "**Features to discard**\n",
    "- Numeric\n",
    "    - PassengerId\n",
    "- Categorical\n",
    "    - Name\n",
    "    - Ticket\n",
    "    - Cabin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cfac6291-33cc-506e-e548-6cad9408623d",
    "_uuid": "73a9111a8dc2a6b8b6c78ef628b6cae2a63fc33f"
   },
   "source": [
    "# Data preparation\n",
    "\n",
    "## 1. Drop irrelevant or less important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "da057efe-88f0-bf49-917b-bb2fec418ed9",
    "_uuid": "e328d9882affedcfc4c167aa5bb1ac132547558c"
   },
   "outputs": [],
   "source": [
    "train_df1 = train_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "test_df1 = test_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "print(train_df1.shape, test_df1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Handle missing values (NaN)\n",
    "Options:\n",
    "- Remove all rows that contain missing values: df.dropna(axis=0)\n",
    "- Remove all columns that contain missing values: df.dropna(axis=1)\n",
    "- **Imputate missing values**\n",
    "    - Categorical feature: \n",
    "        - Replaced with a new unique value (such as \"Unknown\")\n",
    "        - Replaced with the most frequently occuring one\n",
    "    - Numeric feature:\n",
    "        - Replace with median (or mean or mode) of the dataset\n",
    "        - Estimate a more reasonable value for the missing value based on other available features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df1.copy(deep=True)\n",
    "test_df2 = test_df1.copy(deep=True)\n",
    "\n",
    "# Age and Fare\n",
    "age_median = train_df2.Age.median()\n",
    "fare_median = train_df2.Fare.median()\n",
    "print(age_median, fare_median)\n",
    "\n",
    "train_df2[\"Age\"] = train_df2[\"Age\"].fillna(age_median)\n",
    "train_df2[\"Fare\"] = train_df2[\"Fare\"].fillna(fare_median)\n",
    "train_df2[\"Embarked\"] = train_df2[\"Embarked\"].fillna('Unknown')\n",
    "\n",
    "print(train_df2.Embarked.unique())\n",
    "\n",
    "test_df2[\"Age\"] = test_df2[\"Age\"].fillna(age_median)\n",
    "test_df2[\"Fare\"] = test_df1[\"Fare\"].fillna(fare_median)\n",
    "test_df2[\"Embarked\"] = test_df2[\"Embarked\"].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2c8e84bb-196d-bd4a-4df9-f5213561b5d3",
    "_uuid": "a1ac66c79b279d94860e66996d3d8dba801a6d9a"
   },
   "source": [
    "## 3. Convert a categorical features to numerical ones\n",
    "\n",
    "Two Categorical features to convert: *Sex, Embarked*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df3 = train_df2.copy(deep=True)\n",
    "test_df3 = test_df2.copy(deep=True)\n",
    "\n",
    "train_df3['Sex'] = train_df3['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "test_df3['Sex'] = test_df3['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "\n",
    "# Use either mapping or ordinal encoder\n",
    "#train_df3['Embarked'] = train_df3['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2, 'Unknown': 3} ).astype(int)\n",
    "#test_df3['Embarked'] = test_df3['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2, 'Unknown': 3} ).astype(int)\n",
    "\n",
    "enc = preprocessing.OrdinalEncoder(dtype=int)\n",
    "enc.fit(train_df3[['Embarked']])\n",
    "train_df3['Embarked'] = enc.transform(train_df3[['Embarked']])\n",
    "test_df3['Embarked'] = enc.transform(test_df3[['Embarked']])\n",
    "\n",
    "train_df3.info()\n",
    "print(train_df3.Sex.unique())\n",
    "print(train_df3.Embarked.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estimate feature importance based on correlation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_df3.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features in the desceasing order of importance:\n",
    "- Sex (0.543351)\n",
    "- Pclass (-0.338481)\n",
    "- Fare (0.257307)\n",
    "- Embarked (0.118026)\n",
    "- Parch (0.0816294)\n",
    "- age (-0.0816294)\n",
    "- SibSp (-0.0353225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train prediction models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Organize data in pairs of X (features), Y (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df3.drop(\"Survived\", axis=1, inplace=False)\n",
    "Y_train = train_df3[\"Survived\"]\n",
    "X_test = test_df3.copy()\n",
    "X_train.shape, Y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train and evaluate various models\n",
    "### 2.1 Train and evaluate a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a classification model\n",
    "classifier = SVC(gamma='auto') # gamma='scale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate model with the same dataset (bad)\n",
    "classifier.fit(X_train, Y_train)\n",
    "print('Accuracy using SVC:', classifier.score(X_train, Y_train) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate model with the different dataset (ok)\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_train, Y_train, test_size = 0.25)\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n",
    "classifier.fit(train_x, train_y)\n",
    "print('Accuracy using SVC:', classifier.score(test_x, test_y) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-fold cross validation (better)\n",
    "acc = cross_val_score(classifier, X_train, Y_train, cv=4)\n",
    "print(acc, acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cross validate with more classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression() #(max_iter=200)\n",
    "linear_svc = LinearSVC() #(dual='auto')\n",
    "svc = SVC()\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "random_forest = RandomForestClassifier()\n",
    "gboost = HistGradientBoostingClassifier()\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "gaussian = GaussianNB()\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression' : logreg, \n",
    "    'Linear SVC' : linear_svc, \n",
    "    'SVC' : svc, \n",
    "    'Decision Tree' : decision_tree,\n",
    "    'Random Forest' : random_forest, \n",
    "    'HistGradientBoosting' : gboost,\n",
    "    'KNN' : knn, \n",
    "    'Naive Bayes' : gaussian\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_all_models(models, X_train, Y_train, n_folds=4) :\n",
    "    for name, classifier in models.items() :\n",
    "        acc = cross_val_score(classifier, X_train, Y_train, cv=n_folds)\n",
    "        print(name, acc, acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_all_models(models, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Choose the model with the highest accuracy and re-train it with the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost.fit(X_train, Y_train)\n",
    "Y_test = gboost.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": Y_test\n",
    "    })\n",
    "print(submission.head())\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check the accuracy\n",
    "- By submitting \"submission.csv\" on https://www.kaggle.com/competitions/titanic\n",
    "- Compare the predictions with the leaked groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateAccuracy(prediction) :\n",
    "    gt_df = pd.read_csv('leaked-titanic.csv')\n",
    "    gt = gt_df[\"Survived\"].values\n",
    "    #print(gt)\n",
    "    return (np.mean(prediction==gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy evaluated on testing dataset:\", CalculateAccuracy(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve prediction accuracy\n",
    "## Better data preparation\n",
    "### 1. Dig useful information out of Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Name.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df4 = train_df3.copy(deep=True)\n",
    "test_df4 = test_df3.copy(deep=True)\n",
    "\n",
    "train_df4['Title'] = train_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "print(train_df4['Title'].describe())\n",
    "print('-'*80)\n",
    "test_df4['Title'] = test_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "print(test_df4['Title'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df4['Title'].value_counts())\n",
    "print('-'*80)\n",
    "print(test_df4['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature values that have limited appearances cannot provide stable stastistic information\n",
    "# to learn. So, we tag them with a new value \"rare\"\n",
    "train_df4['Title'] = train_df4['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    "                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "train_df4['Title'] = train_df4['Title'].replace('Mlle', 'Miss')\n",
    "train_df4['Title'] = train_df4['Title'].replace('Ms', 'Miss')\n",
    "train_df4['Title'] = train_df4['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "test_df4['Title'] = test_df4['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    "                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "test_df4['Title'] = test_df4['Title'].replace('Mlle', 'Miss')\n",
    "test_df4['Title'] = test_df4['Title'].replace('Ms', 'Miss')\n",
    "test_df4['Title'] = test_df4['Title'].replace('Mme', 'Mrs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df4['Title'].value_counts())\n",
    "print('-'*80)\n",
    "print(test_df4['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "\n",
    "train_df4['Title'] = train_df4['Title'].map(title_mapping)\n",
    "test_df4['Title'] = test_df4['Title'].map(title_mapping)\n",
    "\n",
    "train_df4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df4.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c237b76-d7ac-098f-0156-480a838a64a9",
    "_uuid": "e3d4a2040c053fbd0486c8cfc4fec3224bd3ebb3"
   },
   "source": [
    "### 2. Create new feature \"family size\" or \"travel alone\"\n",
    "\n",
    "FamilySize = Parch + SibSp. \n",
    "Travel alone =  true if FamilySize=0, or false otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7e6c04ed-cfaa-3139-4378-574fd095d6ba",
    "_uuid": "33d1236ce4a8ab888b9fac2d5af1c78d174b32c7"
   },
   "outputs": [],
   "source": [
    "train_df5 = train_df4.copy(deep=True)\n",
    "test_df5 = test_df4.copy(deep=True)\n",
    "\n",
    "train_df5['FamilySize'] = train_df5['SibSp'] + train_df5['Parch']\n",
    "test_df5['FamilySize'] = test_df5['SibSp'] + train_df5['Parch']\n",
    "\n",
    "train_df5[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df5['IsAalone'] = (train_df5['FamilySize']==0).astype(int)\n",
    "test_df5['IsAalone'] = (test_df5['FamilySize']==0).astype(int)\n",
    "\n",
    "train_df5[['IsAalone', 'Survived']].groupby(['IsAalone'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e6b87c09-e7b2-f098-5b04-4360080d26bc",
    "_uuid": "3da4204b2c78faa54a94bbad78a8aa85fbf90c87"
   },
   "source": [
    "Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "74ee56a6-7357-f3bc-b605-6c41f8aa6566",
    "_uuid": "1e3479690ef7cd8ee10538d4f39d7117246887f0"
   },
   "outputs": [],
   "source": [
    "# Keep or discard feature \"FamilySize\"\n",
    "train_df5 = train_df5.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n",
    "test_df5 = test_df5.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 One-hot encoding \"Embarked\" instead of ordinal encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding (to make it take effect, change this cell from Markdown to Code)\n",
    "enc = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "enc.fit(train_df4[['Title']])\n",
    "\n",
    "train_oh_df = pd.DataFrame(enc.transform(train_df4[['Title']]))\n",
    "train_oh_df.index = train_df4.index\n",
    "train_oh_df = train_oh_df.add_prefix('Title_')\n",
    "test_oh_df = pd.DataFrame(enc.transform(test_df4[['Title']]))\n",
    "test_oh_df.index = test_df4.index\n",
    "test_oh_df = test_oh_df.add_prefix('Title_')\n",
    "\n",
    "train_df5 = train_df5.drop(['Title'], axis=1)\n",
    "test_df5 = test_df5.drop(['Title'], axis=1)\n",
    "\n",
    "train_df5 = pd.concat([train_df5, train_oh_df], axis=1)\n",
    "test_df5 = pd.concat([test_df5, test_oh_df], axis=1)\n",
    "\n",
    "train_df5.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 One-hot encoding \"Cabin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc = preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False) \n",
    "enc.fit(train_df[['Cabin']])\n",
    "\n",
    "train_oh_df = pd.DataFrame(enc.transform(train_df[['Cabin']])) \n",
    "train_oh_df.index = train_df5.index \n",
    "train_oh_df = train_oh_df.add_prefix('Cabin_') \n",
    "\n",
    "test_oh_df = pd.DataFrame(enc.transform(test_df[['Cabin']])) \n",
    "test_oh_df.index = test_df5.index \n",
    "test_oh_df = test_oh_df.add_prefix('Cabin_')\n",
    "\n",
    "train_df5 = pd.concat([train_df5, train_oh_df], axis=1) \n",
    "test_df5 = pd.concat([test_df5, test_oh_df], axis=1)\n",
    "\n",
    "train_df5.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick cross validate models with the better prepared data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df5.drop(\"Survived\", axis=1, inplace=False)\n",
    "Y_train = train_df5[\"Survived\"]\n",
    "X_test = test_df5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_all_models(models, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyper-parameters of models\n",
    "### Manual tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=500)\n",
    "linear_svc = LinearSVC(dual='auto', tol=1e-5)\n",
    "svc = SVC(C=500.0, degree=2)\n",
    "#decision_tree = DecisionTreeClassifier()\n",
    "random_forest = RandomForestClassifier(n_estimators=100, max_depth=7)\n",
    "gboost = HistGradientBoostingClassifier(max_depth=7)\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "gaussian = GaussianNB()\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression' : logreg, \n",
    "    'Linear SVC' : linear_svc, \n",
    "    'SVC' : svc, \n",
    "    'Decision Tree' : decision_tree,\n",
    "    'Random Forest' : random_forest, \n",
    "    'HistGradientBoosting' : gboost,\n",
    "    'KNN' : knn, \n",
    "    'Naive Bayes' : gaussian\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_all_models(models, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-search tuning top 3 classifiers\n",
    "#### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "parameters = {'n_estimators':[120, 150, 180], 'max_depth':[7, 9, 11]}\n",
    "gs = GridSearchCV(random_forest, parameters, cv=4)\n",
    "gs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test1 = gs.best_estimator_.predict(X_test)\n",
    "print(\"Accuracy evaluated on testing dataset:\", CalculateAccuracy(Y_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth':[7, 9, 11], 'l2_regularization':[0.3, 0.5], 'max_iter':[70], 'max_leaf_nodes':[20, 35, 50]}\n",
    "gs = GridSearchCV(gboost, parameters, cv=4)\n",
    "gs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test2 = gs.best_estimator_.predict(X_test)\n",
    "print(\"Accuracy evaluated on testing dataset:\", CalculateAccuracy(Y_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=500.0, degree=2)\n",
    "parameters = {'C':[400.0, 500.0, 600.0], 'degree':[2, 3, 4]}\n",
    "gs = GridSearchCV(svc, parameters, cv=4)\n",
    "gs.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test3 = gs.best_estimator_.predict(X_test)\n",
    "print(\"Accuracy evaluated on testing dataset:\", CalculateAccuracy(Y_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collective voting (does not improve anything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y_test1 + Y_test2 + Y_test3\n",
    "Y_test = (Y_test>1).astype(int)\n",
    "print(\"collective voting accuracy:\", CalculateAccuracy(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible ways to further improve the results\n",
    "* Treat categorical features as 'category' type\n",
    "* Use One-hot encoded 'Cabin' feature\n",
    "* Feature engineering\n",
    "    * Binning\n",
    "    * Scaling (min-max or standard)\n",
    "    * PCA\n",
    "* Grid-search for more hyper-parameters \n",
    "* Try other models\n",
    "    * Catboost\n",
    "    * XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    },
    {
     "datasetId": 677541,
     "sourceId": 1190700,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 20477,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "2024-ml Kernel",
   "language": "python",
   "name": "2024-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
